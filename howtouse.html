<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Thunder by freeman-lab</title>
    <link rel="stylesheet" href="stylesheets/bootstrap.min.css">
    <link rel="stylesheet" href="http://yandex.st/highlightjs/7.5/styles/tomorrow-night.min.css">
    <link rel="stylesheet" href="stylesheets/style.css">
    <link href="http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic" rel="stylesheet" type="text/css">
    <script src="http://yandex.st/highlightjs/7.5/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <script src="javascripts/bootstrap.min.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      
      <div class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="container">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="navbar-collapse collapse">
            <ul class="nav navbar-nav">
              <li><a id="menu" href="index.html">Overview</a></li>
              <li class="active"><a id="menu" href="howtouse.html">How to use</a></li>
              <li><a id="menu" href="https://github.com/freeman-lab/thunder/archive/master.zip">Download</a></li>
              <li><a id="menu" href="https://github.com/freeman-lab/thunder">View on github</a></li>
            </ul>
          </div><!--/.nav-collapse -->
        </div>
      </div>


      <div class="container">
        <div class="row">
          <div class="col-md-10">
            <article id = "docs">
              <div class="tab-content">
                <div class="tab-pane fade in active" id="InstallationBasic">
<h1 id="doctitle">Installation (basic)</h1>
<p>This is a guide to installing Thunder. It assumes you’ve spent some time in a terminal. It's been tested on Mac OS X, but should also work on Linux. It will let you run Spark and Thunder on your local machine, which is useful for understanding how it works. See Installation (cluster) for cluster usage.</p>
<p>First, you need to set up Spark. We'll go through the basics here, but consult the <a href="http://spark.incubator.apache.org/docs/latest/">documentation</a> for more details. Download from the <a href="http://spark.incubator.apache.org/downloads.html">project page</a>, and download Scala from <a href="http://www.scala-lang.org/download/2.9.3.html">scala-lang.org</a>. Put them anywhere on your file system. Set paths (by typing these lines into the terminal, or by adding them to your .bash_profile).</p>
<br>
<pre><code class="bash">export SCALA_HOME=your_path_to_scala
export SPARK_HOME=your_path_to_spark
</pre></code>
<br>
<p>Go into the top-level Spark directory and build Spark.</p>
<br>
<pre><code class="bash">cd $SPARK_HOME
sbt/sbt assembly
</pre></code>
<br>
<p>This might take a few minutes. When done, to check if Spark / Pyspark is working, start the Pyspark shell</p>
<br>
<pre><code class="bash">$SPARK_HOME/pyspark
</pre></code>
<br>
<p>And do a simple operation</p>
<br>
<pre><code class="python">>> test = sc.parallelize([1,2,3])
>> test.count()
>> # should return 3
>> exit()
</pre></code>
<br>
<p>If that didn’t work, consult the Spark <a href="http://spark.incubator.apache.org/docs/latest/>documentation">documentation</a>.</p>
<p>Thunder also relies on Python and its libraries for scientific computing, Numpy and Scipy. Read <a href="http://www.lowindata.com/2013/installing-scientific-python-on-mac-os-x/">this post</a> for help setting up scientific python, or install a free prepackaged version like <a href="https://store.continuum.io/cshop/anaconda/">anaconda</a>. To make sure everything’s working, run</p>
<br>
<pre><code class="python">python
>> import numpy
>> import scipy
</pre></code>
<br>
<p>That should start Python and import Numpy and Scipy. Type <samp id="inline">exit()</samp> to quit.</p>
<p>Now we’re ready to install Thunder. Download the latest <a href="https://github.com/freeman-lab/thunder/archive/master.zip">build</a>, and add it to your path (again, it’s useful to add these lines to your .bash_profile).</p>
<br>
<pre><code class="bash">export PYTHONPATH=your_path_to_thunder/python/:$PYTHONPATH
</pre></code>
<br>
<p>Go into the top-level Thunder directory and run an analysis on test data.</p>
<br>
<pre><code class="bash">$SPARK_HOME/pyspark python/thunder/factorization/pca.py local data/iris.txt ~/results 4
</pre></code>
<br>
<p>This will run principal component analysis and save the results to your home directory in the folder results-pca. The <samp id="inline">local</samp> tells Spark to run in local mode (i.e. not on a cluster). This is useful for debugging purposes. If you are on a multi-core machine, use <samp id="inline">local[n]</samp> to use n cores.</p>
<p>The same analysis can be run interactively. Start the Pyspark shell</p>
<br>
<pre><code class="bash">$SPARK_HOME/pyspark
</pre></code>
<br>
<p>Then run the analysis and inspect the results.</p>
<br>
<pre><code class="python">>> from thunder.util.dataio import parse
>> from thunder.factorization.pca import pca
>> lines = sc.textFile('data/iris.txt')
>> data = parse(lines).cache()
>> scores, latent, comps = pca(data, 4)
>> latent
>> # 1x4 numpy array
>> comps
>> # 4x4 numpy array
>> scores.collect()
>> # RDD with 150 elements
</pre></code>
<br>
<p>Running analyses interactively this way is an especially useful way to learn how they work, debug them, and develop new ones.</p>
                </div>
                <div class="tab-pane fade" id="InstallationCluster">
<h1 id="doctitle">Installation (cluster)</h1>
<h2 id="docsubtitle">Instructions for Amazon's EC2</h2>
<p>Spark’s deploy scripts make it easy to get up and running on Amazon’s EC2 cloud computing services. First install Spark by reading Installation (basic) or the <a href="http://spark.incubator.apache.org/docs/latest/">Spark documentation</a>, then follow <a href="http://spark.incubator.apache.org/docs/latest/ec2-scripts.html">these instructions</a> up through the point where you log in to the cluster’s master node.</p>
<p>Once logged in to the master, download Thunder to the root directory (if you put it elsewhere, change the paths below accordingly)</p>
<br>
<pre><code class="bash">wget https://github.com/freeman-lab/thunder/archive/master.zip
unzip master.zip
export PYTHONPATH=/root/thunder-master/python/:$PYTHONPATH</pre></code>
<br>
<p>Go into the top-level directory and build an egg</p>
<br>
<pre><code class="bash">cd /root/thunder/python
./setup.py bdist_egg
export THUNDER_EGG=/root/thunder/python/dist/</pre></code>
<br>
<p>This will allow Pyspark to send Thunder's libraries to the worker nodes without explicitly installing Thunder on each one. You also need to execute a helper script that installs Numpy and Scipy on the workers.</p>
<br>
<pre><code class="bash">chmod +x /root/thunder/helper/copy-numpy-scipy-ec2.sh
/root/thunder/helper/copy-numpy-scipy-ec2.sh</pre></code>
<br>
<p>Now Thunder is ready to run, but you’ll probably want some data to analyze. If your data is stored in Amazon’s S3, copy it into HDFS by running</p>
<br>
<pre><code class="bash">/root/ephemeral-hdfs/bin/start-all.sh
/root/ephemeral-hdfs/bin/hadoop distcp s3n://your_aws_access_key:your_aws_secret_access_key@your_s3_bucket_name/your_file_name hdfs:///data
</pre></code>
<br>
<p>This step may take a while, especially for larger data sets. Once it’s done, set a couple more paths:</p>
<br>
<pre><code class="bash">export MASTER=spark://$SPARK_URL:7077
export DATA=hdfs://$SPARK_URL:9000/data
</pre></code>
<br>
<p>Then go into the top-level Thunder directory and run an analysis</p>
<br>
<pre><code class="bash">$SPARK_HOME/pyspark python/thunder/factorization/pca.py $MASTER $DATA ~/results 4
</pre></code>
<br>
<p>At this point, you should be able to run any of the analyses in Thunder!</p>
<p>If you want to run in interactive mode, just start the shell</p>
<br>
<pre><code class="bash">$SPARK_HOME/pyspark
</pre></code>
<br>
<p>Make sure that <samp id="inline">MASTER</samp> is set correctly (see above), otherwise Pyspark might be running in local mode even though you are on a cluster. At the start of an analysis, you'll want to import any packages from thunder that you'll need, for example</p>
<br>
<pre><code class="python">>> from thunder.factorization.pca import pca
>> from thunder.factorization.ica import ica
</pre></code>
<br>
<p>You will also want to add Thunder's egg to the SparkContext</p>
<br>
<pre><code class="python">>> import glob, os
>> egg = glob.glob(os.environ['THUNDER_EGG'] + "*.egg")
>> sc.addPyFile(egg)
</pre></code>
<br>

<h2 id="docsubtitle">Instructions for a private cluster</h2>
<p>To run Thunder on a private cluster, you’ll first need to set up Spark. The simplest option is to set up in Standalone mode, following <a href="http://spark.incubator.apache.org/docs/latest/spark-standalone.html">these instructions</a>. I recommend giving this information to a systems administrator (or whoever manages the cluster), they should be able to help.</p>
<p>Once Spark is up and running, you can run Thunder by downloading and installing it on the master node (as discussed above). Remember to provide the IP address of the master when starting an analysis in Thunder, either by setting the environmental variable <samp id="inline">MASTER</samp> (for running in interactive mode) or by providing it as an argument to an analysis script. Make sure that Numpy and Scipy are installed on all the worker nodes, and that the data is available to all the worker nodes (via HDFS or another networked-file-system).</p>
                </div>


                <div class="tab-pane fade" id="InputFormat">
<h1 id="doctitle">Input format</h1>
<p>Description of input format</p>
                </div>


                <div class="tab-pane fade" id="OutputFormat">
<h1 id="doctitle">Output format</h1>
<p>Description of availiable output formats</p>
                </div>
                

                <div class="tab-pane fade" id="Analyses">
<h1 id="doctitle">Analyses</h1>
<p>Thunder includes four analysis packages: clustering, factorization, regression, and signal processing, as well as utils for shared methods. Each package has stand-alone scripts that perform common analysis routines, alongside loading data and saving results. But the main functions in each script, as well as any shared algorithms and classes, can be called directly from within the PySpark shell (for interactive use), or from other scripts (when developing your own analyses). Here, we explain the main functionality through example calls to these functions. We assume <samp id="inline">data</samp> is an RDD where each element is a numpy array, and possibly a key identifier (see Input format for more details). The neccessary functions must be imported before use.</p>
<h2 id="docsubtitle">Clustering</h2>
<p>Analyses: kMeans</p>
<p>Clustering analyses try to associate data points with particular categories based on a distance metric. kMeans assigns each data point to one of k clusters using an iterative algorithm. To run, call</p>
<br>
<pre><code class="python">from thunder.clustering.kmeans import kmeans
labels, centers = kmeans(data, k, maxiter, tol)
</pre></code>
<br>
<p>Where k is the number of clusters, maxiters is the maximum number of iterations, and tol is the tolerance for change in cluster estimates. Labels is an RDD with the assigned label of each data point, and clusters is a numpy array with the estimated cluster centers.</p>
<br>
<h2 id="docsubtitle">Factorization</h2>
<p>Analyses: Principal Component Analysis, Independent Component Analysis</p>
<p>Factorization methods separate or decompose a data matrix into smaller, lower-rank matrices by optimizing an objective function. PCA finds a decomposition that minimizes the squared error between the true data matrix and the matrix reconstructed from the low-rank matrices. To run PCA, call</p>
<br>
<pre><code class="python">from thunder.factorization.pca import pca
scores, latent, comps = pca(data, k, svdmethod)
</pre></code>
<br>
<p>Where k is the number of principal components to return, and svdmethod specifies which SVD method to use ('direct' or 'em', see below). comps is an array with the principal components, latent is an array with the variance explained by each component, and scores is an RDD with the data projected into the principal component space.</p>
<p>The source model for PCA is that the data is Gaussian-distributed with a low-rank covariance determined by the components. ICA, in contrast, assumes that the components and non-Gaussian and statistically independent from one another, and estimates them by iteratively maximizing an objective function that computes the non-Gaussanity of the components. As is common, Thunder's ICA first reduces the dimensionality of the data using PCA. To run ICA, call</p>
<br>
<pre><code class="python">from thunder.factorization.ica import ica
w, sigs = ica(data, k, c, svdmethod, maxiter, tol)
</pre></code>
<br>
<p>Where k is the reduced dimensionality to use, c is the number of independent components to find, svdmethod is which SVD to use to do the initial PCA, and maxiter and tol control the iterative ICA algorithm.</p>
<p>Both PCA and ICA rely on a large-scale singular value decomposition (SVD), which has two variants. One is direct: it maps the outer product of all time series data, and then adds the results through an accumulator. This is highly efficient for many data points with few time points (e.g. a "tall and skinny" matrix), but for longer time series data it will become inefficient. So Thunder also includes an iterative method using expectation maximization. To call the SVD directly call</p>
<br>
<pre><code class="python">from thunder.factorization.utils import svd
scores, latent, comps = svd(data, k, meansubtract, svdmethod)
</pre></code>
<br>
<p>Inputs are similar to those for PCA, with the additional specification of whether or not to subtract the mean of each data point (0 or 1).</p>
<h2 id="docsubtitle">Regression</h1>
<p>Analyses: regress (linear, bilinear), Tuning (gaussian, circular)
<p>regression models describe each time series as a function of some underlying variables. Thunder includes both linear and bilinear regression, called by:</p>
<br>
<pre><code class="python">from thunder.regression.utils import RegressionModel
model = RegressionModel.load(modelfile, regressmode)
betas, stats, resid = model.fit(data)
</pre></code>
<br>
<p>modelfile is an array, tuple of arrays, or a string with the name of a file, specifying the parameters to regress each time series against (often called a design matrix). regressmode is the form of regression: current options are “linear”, “bilinear”. “linear” fits each time series as a linear combination of some variables, plus a constant. “bilinear” uses two design matrix; it uses the first to estimate a basis function that is common to each of several conditions, and uses the second to describe each condition as a scaled version of that basis function.</p>
<p>betas, stats, and resid are RDDs with the regression results. betas contains the coefficients, stats contains the r2 value for each data point, and resid contains the residuals. It's easy to pass these values to other algorithms, for example, to do PCA and kMeans on the coefficients.</p>
<br>
<pre><code class="python">scores, latent, comps = pca(coeffs, 2)
labels, centers = kmeans(coeffs, 2)
</pre></code>
<br>
<p>tuning models estimate the parameters of a tuning curve that relate an input value to an output value. Tuning curves are estimated by:</p>
<br>
<pre><code class="python">from thunder.regression.utils import TuningModel
model = TuningModel.load(modelfile, tuningmode)
params = model.fit(data)
</pre></code>
<br>
<p>Where modelfile is an array or string with the name of a file specifying the  input value corresponding to the entries of each data point.</p>
<h2 id="docsubtitle">Signal processing</h1>
<p>Analyses: Stats, Fourier, Cross Correlation, Local Correlation, Query</p>
<p>Signal processing analyses compute statistics on data (typically time series data), either simple summary statistics, or the results of more complex signal processing. Stats is for computing summary statistics, called by:</p>
<br>
<pre><code class="python">from thunder.stats.utils import SigProcessingMethod
method = SigProcessingMethod.load('stats', statistic)
out = method.calc(data)
</pre></code>
<br>
<p>statistic can be 'mean', 'median', 'std', or 'norm', and out is an RDD with the value of that statistic for each data point.</p>
<p>fourier estimates the statistics of the Fourier transform, specifically, the amplitude and phase of the time series at the specified frequency. Call it by:</p>
<br>
<pre><code class="python">from thunder.stats.utils import SigProcessingMethod
method = SigProcessingMethod.load('fourier', freq)
out = method.calc(data)
</pre></code>
<br>
<p>Where freq is the frequency (total number of cycles per time series minus 1). out is an RDD of tuples, each containing the coherence and phase for a data point. (coherence is Fourier amplitude normalized by the amplitude of all frequencies, yielding a number between 0 and 1 that is more interpretable than raw amplitude.)</p>
<p>crosscorr computes the cross correlation between data and a signal of interest at multiple time lags. Call it by:</p>
<br>
<pre><code class="python">from thunder.stats.utils import SigProcessingMethod
method = SigProcessingMethod.load('crosscorr', sigfile, lag)
betas = method.calc(data)
</pre></code>
<br>
<p>Where sigfile is an array or string with a file name specifying the signal to correlate against, and lag is the maximum temporal lag to consider. betas is an RDD containing the cross-correlation function for each data point (of size 2 x lag + 1). If lag is large, this will still be a big data set, so it's often useful to do PCA on betas</p>
<p>For spatio-temporal data, localcorr estimates the correlation between each time series and the average time series of its neighbors, useful for detecting structures with locally correlated activity.</p>
<br>
<pre><code class="python">corrs = localcorr(data, sz)
</pre></code>
<br>
<p>Where sz is the neighborhood size used in computing the local averages. This method assumes that data is an RDD of (Tuple, Array) pairs where Tuple contains the x,y,z coordinates of each data point, and Array is its time series.</p>
<p>query is a method for quickly extracting local averages from spatio-temporal data, by specifying a set of indices to include in the average, called by:</p>
<br>
<pre><code class="python">ts = query(data, indsfile)
</pre></code>
<br>
<p>Where indsfile is an array of arrays containing the different sets of indices, or the name of a file. This method assumes that data is an RDD of (Int, Array) pairs where the Int is a linear index for each data point (based on its x,y,z coordinates), and Array is its time series.</p>
                </div>


                <div class="tab-pane fade" id="FAQ">
<h1 id="doctitle">FAQ</h1>
<p>Currently empty. Contact the.freeman.lab@gmail.com with questions! Or post an issue to the <a href="https://github.com/freeman-lab/thunder/issues">project page</a> on github.</p>
                </div>


              </div>
            </article>
          </div>
          <div class="col-md-2">
            <ul class="nav nav-pills nav-stacked">
              <li><a href="#InstallationBasic" data-toggle="tab">Installation (basic)</a></li>
              <li><a href="#InstallationCluster" data-toggle="tab">Installation (cluster)</a></li>
              <li><a href="#InputFormat" data-toggle="tab">Input format</a></li>
              <li><a href="#OutputFormat" data-toggle="tab">Output format</a></li>
              <li><a href="#Analyses" data-toggle="tab">Analyses</a></li>
              <li><a href="#FAQ" data-toggle="tab">FAQ</a></li>
            </ul>
          </div>
        </div>
      </div>

<footer>
      <div class="container">
        <p id="builtby">Built by <a href="https://github.com/freeman-lab/">The Freeman Lab</a> and <a href="https://github.com/freeman-lab/thunder/graphs/contributors">contributers</a></p>
      </div>
    </footer>

    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-46696594-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>