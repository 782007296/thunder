
.. \_regression\_tutorial:

.. currentmodule:: thunder

Regression
==========

Linear regression is a simple and powerful tool for describing
relationships between a response variable and one or more explanatory
variables. Here, we show how to peform ordinary least squres regression
in Thunder and visualize the results.

Setup plotting
--------------

.. code:: python

    %matplotlib inline
    import matplotlib.pyplot as plt
    import seaborn as sns
    from thunder import Colorize
    show = Colorize.show
    sns.set_context('notebook')
    sns.set_style('darkgrid')
Load and convert data
---------------------

Load a series of images and cache the data to speed up subsequent
operations.

.. code:: python

    data = tsc.loadExample('mouse-images')
Take a look at the first image.

.. code:: python

    show(data.values().first())


.. image:: regression_files/regression_9_0.png


Convert the Images data to TimeSeries data, apply preprocessing, and
cache the results. Here, the preprocessing does two things: squelches
the data (to set all signals that do not exceed a threshold to 0, which
eliminates noise pixels), and normalizes (subtracting and removing all
time series by a baseline).

.. code:: python

    series = data.toSeries().toTimeSeries().squelch(50).normalize()
    series.cache()
    series.count();
Take a look at the time series of a few pixels (selected to have large
standard devaitions to avoid pixels with no activity).

.. code:: python

    sample = series.subset(100, thresh=0.8)
    plt.plot(sample.T);


.. image:: regression_files/regression_13_0.png


Load in associated behavioral data to be used as explanatory variables
(a.k.a regressors) in predicting the above time series.

.. code:: python

    regressors = tsc.loadExample('mouse-params')
    regressors



.. parsed-literal::

    Params
    names: ['runningSpeed', 'corridorPosition']



Correlation analysis
--------------------

The correlation coefficient provides a measure of the strength of the
linear relationship between two variables. When we have only a single
explanatory varaible, the correlation coefficient offers a simpler
alternative to a regression analysis. Let's start by correlating the
activity from each pixel with one of our explanatory variables.

.. code:: python

    x = regressors['runningSpeed']
    corrs = series.correlate(x)
We can pack these correlation coefficients back into an arrary and
visualize the results. We center the values of the correlation
coefficients so that positive correlations are red and negative
correlations are blue.

.. code:: python

    def center(m):
        y = m.copy()
        y[y>0] = y[y>0]/max(y[y>0])
        y[y<0] = y[y<0]/-min(y[y<0])
        return y
    
    corrMat = center(corrs.pack())
    show(corrMat, bar=True, cmap='RdBu_r')


.. image:: regression_files/regression_20_0.png


Regression analysis
-------------------

With more than two explanatory variables, correlation is no longer a
viable optoin, so we turn to linear regression. Linear regression in
Thunder models the data as:

:math:`y=\beta X`

where :math:`y` is a row-vector of measurements of the response
variable, :math:`\beta` is a row-vector of regression coeffients, one
for each regressor, and :math:`X` is a design matrix with one row for
each regressor and one column for each measurement

Since our data are time series, the columns of :math:`y` and :math:`X`
represent measurements at different instants in time. Note: Thunder
automatically adds a row of 1's to the design matrix to model an
intercept term.

.. code:: python

    X = regressors[['runningSpeed', 'corridorPosition']]
Unlike the correlation coefficient, linear regression is sensitive to
the scale of the regressor. Thus, we standardize our regressors so that
the resulting regression coefficients can be compared on the same scale.
This will be useful later for visualization of the results.

.. code:: python

    from scipy.stats.mstats import zscore
    X = zscore(X, axis=1)
For each pixel/series fit a linear regression model that predicts the
pixel values from our regressors

.. code:: python

    from thunder import RegressionModel
    results = RegressionModel.load(X, 'linear').fit(series)
The result is a Series with three indices: + "betas": the regression
coefficients + "stats": :math:`R^2` value + "resid": the residuals

We can collect the regression coefficients and visualize how they are
distribution across the image.

.. code:: python

    b1, b2 = results.select('betas').pack()
    rsq = results.select('stats').pack()
.. code:: python

    show(rsq, bar=True)


.. image:: regression_files/regression_30_0.png


.. code:: python

    b1map, b2map = center(b1), center(b2)
    
    plt.subplots(1, 3, figsize=(15 ,10))
    plt.subplot(1, 3, 1)
    show(b1map, cmap='RdBu_r')
    plt.title('b1')
    plt.subplot(1, 3, 2)
    show(b2map, cmap='RdBu_r');
    plt.title('b2')
    plt.subplot(1, 3, 3)
    show(rsq)
    plt.title('r-squared');


.. image:: regression_files/regression_31_0.png


To combine these sources of information, we will make composite maps,
one for the first regressor, one for the second, and one for both. In
each map, we'll pick a color to represent the regressor, and use the
absolute value to represent all pixels that are related to that
variable, whether positively or negative. We will also use :math:`R^2`
as a mask, and use the mean image as a background to blend in.

.. code:: python

    m = data.mean()
.. code:: python

    c = Colorize(cmap='indexed', scale=5)
    
    plt.figure(figsize=(20,10))
    plt.subplot(1,3,1)
    c.colors = ['green']
    img = c.transform([abs(b1)], mask=rsq, background=m, mixing=0.75)
    show(img)
    
    plt.subplot(1,3,2)
    c.colors = ['red']
    img = c.transform([abs(b2)], mask=rsq, background=m, mixing=0.75)
    show(img)
    
    plt.subplot(1,3,3)
    c.colors = ['green', 'red']
    img = c.transform([abs(b1), abs(b2)], mask=rsq, background=m, mixing=0.75)
    show(img)


.. image:: regression_files/regression_34_0.png

